{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to import the different modules used in this notebook. In order to download any that aren't already installed, please run \"pip install [module name]\" in a new cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "#https://google.github.io/mediapipe/solutions/hands.html\n",
    "import cv2\n",
    "#https://opencv.org/\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import webbrowser\n",
    "\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "#https://github.com/AndreMiras/pycaw\n",
    "\n",
    "import autopy\n",
    "import pynput\n",
    "from pynput.keyboard import Key, Controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, OpenCV will detect video from your webcam, which MediaPipe will use to detect the following 20 hand landmarks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://google.github.io/mediapipe/images/mobile/hand_landmarks.png' width=\"900\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image from: https://google.github.io/mediapipe/solutions/hands.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following two cells before moving on to part 2 or 3 of the notebook. \n",
    "\n",
    "The cell below has all the gestures the gestures recognition model has been trained on, as well as all the gesture recognition events in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of all the gestures the gesture recognition model is currently trained on:\n",
    "labels = {\n",
    "    0: 'open palm',\n",
    "    1: 'soft palm',\n",
    "    2: 'palm rotated inward',\n",
    "    3: 'fist',\n",
    "    4: 'fist inward',\n",
    "    5: 'pinch',\n",
    "    6: 'open pinch',\n",
    "    7: 'thumbs up',\n",
    "    8: 'thumbs down',\n",
    "    9: 'spock',\n",
    "    10: 'heart',\n",
    "    11: 'ily',\n",
    "    12: 'shaka',\n",
    "    13: 'metal',\n",
    "    14: 'one finger up',\n",
    "    15: 'two fingers up',\n",
    "    16: 'three fingers up',\n",
    "    17: 'four fingers up',\n",
    "    18: 'up pinch one',\n",
    "    19: 'up pinch two',\n",
    "    20: 'up pinch three',\n",
    "    21: 'up pinch four',\n",
    "    22: 'mid pinch one',\n",
    "    23: 'mid pinch two',\n",
    "    24: 'mid pinch three',\n",
    "    25: 'mid pinch four'\n",
    "}\n",
    "\n",
    "# Computer events assigned to different gestures:\n",
    "palm = [0,1]\n",
    "mouse_gesture = palm\n",
    "stop_gesture = [3]\n",
    "volume_gesture = [5,6]\n",
    "end_gesture = [9]\n",
    "zoom_gesture = [11]\n",
    "scroll_gesture = [12]\n",
    "\n",
    "alt_tab_gesture = [2]\n",
    "pause_gesture = [10]\n",
    "tab1_gesture = [14]\n",
    "tab2_gesture = [15]\n",
    "tab3_gesture = [16]\n",
    "tablast_gesture = [17]\n",
    "open_tab_gesture = [20]\n",
    "close_tab_gesture = [21]\n",
    "forward_tab_gesture = [19]\n",
    "backward_tab_gesture = [18]\n",
    "browser_forward_gesture = [18]\n",
    "browser_back_gesture = [19]\n",
    "link1_gesture = [22]\n",
    "link2_gesture = [23]\n",
    "link3_gesture = [24]\n",
    "link4_gesture = [25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell returns the x, y, and z coords the hand landmarks relative to the wrist's coordinates and flattens them so that they can then be fed into a gestures recognition model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_coords(hand):\n",
    "    rel_coords = []\n",
    "    \n",
    "    for lmk in hand.landmark:\n",
    "        coords = (hand.landmark[0].x- lmk.x, \n",
    "                hand.landmark[0].y - lmk.y,\n",
    "                hand.landmark[0].z - lmk.z)\n",
    "        rel_coords.append(coords)\n",
    "    \n",
    "    rel_coords = (np.array(rel_coords)).flatten('F')\n",
    "            \n",
    "    return rel_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting coordinates for gestures from your webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialize an empty list for all the training data and how long you would like each gesture recording to take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "training_time = 20 # For how long the current gesture will be recorded (in seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, set the label for what gesture you would currently like to collect coordinates for. Run the following cell each time you pick a new gesture. When I trained the model, for each gesture, I recorded one set of gestures for my right hand, and then for my right hand, and then moved on to the next gestures label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_label = 26\n",
    "print('Ready to record the following gesture: ' + labels[curr_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will run your webcam and will store the collected coords in coords_list_both. OpenCV handles the video capture. The OpenCV window will close by itself once the set training_time has passed, or earlier if you press 'q'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) #might have to change this if you have more than one webcam\n",
    "cw, ch = 640, 480 #camera resolution\n",
    "sw, sh = autopy.screen.size() #screen resolution\n",
    "cap.set(3,cw)\n",
    "cap.set(4,ch)\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "mp_hands = mp.solutions.hands \n",
    "coords_list_both = []\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5, max_num_hands=1) as hands: \n",
    "    # May need to reduce min_detection_confidence when recording certain gestures\n",
    "    # Only one hand is intended to be noted down per gesture recording for training, so the maximum number of hands is 1\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "    \n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  \n",
    "        image = cv2.flip(image, 1) \n",
    "        image.flags.writeable = False \n",
    "        results = hands.process(image) \n",
    "        image.flags.writeable = True \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "        \n",
    "        if results.multi_hand_landmarks: \n",
    "            for hand, side in zip(results.multi_hand_landmarks, results.multi_handedness): \n",
    "                # Draw all hand landmarks:\n",
    "                mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS,\n",
    "                                          mp_drawing.DrawingSpec(color=(255,255,255), thickness=1, circle_radius=5), \n",
    "                                          mp_drawing.DrawingSpec(color=(255,255,255), thickness=1), \n",
    "                                          )\n",
    "                #####################################################################\n",
    "                handedness = side.classification[0].label                 \n",
    "                if side.classification[0].score > 0.9995: #reduces noise   \n",
    "                     \n",
    "                    if handedness == 'Left':      \n",
    "                        coords_list = get_relative_coords(hand)\n",
    "                        coords_list_both.append(coords_list) #could split up right and left hand for training\n",
    "\n",
    "                    if handedness == 'Right':\n",
    "                        coords_list = get_relative_coords(hand)\n",
    "                        coords_list[:20] = -coords_list[:20] #the right hand is treated as a flipped left hand for the sake of the model    \n",
    "                        coords_list_both.append(coords_list)\n",
    "                \n",
    "                ##################################################################### \n",
    "                          \n",
    "        cv2.imshow('Gestures',image)           \n",
    "        if (cv2.waitKey(10) & 0xFF == ord('q'))|((time.time() - start_time) > training_time): \n",
    "            break\n",
    "                \n",
    "cap.release() \n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "coords_list_both = np.insert(coords_list_both, 0, curr_label, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell for each gesture recording or add the code to the previous cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.append(coords_list_both) \n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to put your data in a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_training_data =[x for sublist in training_data for x in sublist]\n",
    "df = pd.DataFrame(flattened_training_data, columns = ['Label', *list(range(0, 63))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following cell if you would like to append your collected coords to mine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = df\n",
    "# df = (pd.read_csv('training_data.csv')).append(new_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to save your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'training_data_new'\n",
    "df.to_csv(filename+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the gestures recognition neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to prepare a custom dataset from the training data that was just collected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "class HandGesturesDataset(Dataset):\n",
    "    def __init__(self, ):\n",
    "        xy = np.loadtxt(open('training_data.csv','rb'), delimiter=',', dtype=np.float32, skiprows=1)\n",
    "        self.x = torch.from_numpy(xy[:,1:])\n",
    "        self.y = (torch.from_numpy(xy[:,0])).type(torch.LongTensor)\n",
    "        self.n_samples = xy.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "dataset = HandGesturesDataset()\n",
    "indices = list(range(len(dataset)))\n",
    "split = int(np.floor(0.2 * len(dataset)))\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to initialize the neural network used in this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_size = 63\n",
    "num_classes = len(labels)\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "class NeuralNet(pl.LightningModule):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(256, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        return out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"loss\", loss)        \n",
    "        return loss\n",
    "  \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log('val_loss', loss) \n",
    "           \n",
    "    def train_dataloader(self):\n",
    "        train_dataloader = DataLoader(dataset=dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataloader = DataLoader(dataset=dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "        return val_dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to train the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "trainer = Trainer(auto_lr_find=True, max_epochs=num_epochs, gpus=1, fast_dev_run=False)\n",
    "model = NeuralNet(input_size, num_classes)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment this if you would like to run TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %load_ext tensorboard\n",
    "# %tensorboard --logdir ./lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment this if you would like to save your model's weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'gestures_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running the model for gesture recognition and gesture recognition events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the neural network if you haven't done so already in part 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_size = 63\n",
    "num_classes = len(labels)\n",
    "\n",
    "class NeuralNet(pl.LightningModule):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(256, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following cell if you would like to use the pretrained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = NeuralNet(input_size, num_classes)\n",
    "# model.load_state_dict(torch.load('100epochs_simpler.pth')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few general helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feeds hand coordinates through the trained model to get its predicted hand gesture\n",
    "def get_gesture(coords_list):\n",
    "    model_out = model(torch.tensor(coords_list.astype(np.float32)))\n",
    "    gesture = torch.argmax(model_out).item()\n",
    "    return gesture\n",
    "\n",
    "\n",
    "# Returns a distance in 2D or in 3D between two points\n",
    "def get_distance(a, b, hand, xyz=True):\n",
    "    distance = 0\n",
    "    if xyz == True:\n",
    "        distance = np.cbrt((hand.landmark[a].x - hand.landmark[b].x)**2 \n",
    "                           + (hand.landmark[a].y - hand.landmark[b].y)**2\n",
    "                           + (hand.landmark[a].z - hand.landmark[b].z)**2)\n",
    "    else:\n",
    "        distance = np.sqrt((hand.landmark[a].x - hand.landmark[b].x)**2 \n",
    "                           + (hand.landmark[a].y - hand.landmark[b].y)**2)\n",
    "    return distance\n",
    "\n",
    "# Returns gestures if they match ones recorded in previous time steps (to to be able to modulate how responsive the different gesture events should be)\n",
    "def get_gesture_confirmation(gestures):\n",
    "    short, med, long = None, None, None\n",
    "    if gestures[-1] == gestures[-2]:\n",
    "        short = gestures[-1]\n",
    "        if gestures[-1] == gestures[-2] == gestures[-3]:\n",
    "            med = gestures[-1]\n",
    "            if gestures[-1] == gestures[-2] == gestures[-3] == gestures[-4]:\n",
    "                long = gestures[-1]\n",
    "    return short, med, long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for running different hand gesture events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code for the hand mouse, particularly the smoothing and the clicking mechanism is based on the code \n",
    "# from this tutorial: https://www.youtube.com/watch?v=8gPONnGIPgw\n",
    "def hand_mouse(hand, image, prev_x, prev_y):\n",
    "    smoothing = 3\n",
    "    bounds_x = cw/3\n",
    "    bounds_y = ch/3\n",
    "    \n",
    "    x, y = hand.landmark[9].x, hand.landmark[9].y\n",
    "    \n",
    "    x = np.interp(x*cw, (bounds_x,cw-bounds_x),(0,sw))\n",
    "    y = np.interp(y*ch, (bounds_y,ch-bounds_y),(0,sh))\n",
    "\n",
    "    curr_x = prev_x + (x - prev_x)/smoothing\n",
    "    curr_y = prev_y + (y - prev_y)/smoothing\n",
    "    \n",
    "    autopy.mouse.move(curr_x, curr_y)\n",
    "\n",
    "    click_distance = (get_distance(8, 12, hand,xyz=False) / get_distance(5, 8, hand, xyz=False))\n",
    "    if click_distance < 0.2: \n",
    "        autopy.mouse.click()\n",
    "\n",
    "    return curr_x, curr_y\n",
    "####################################################################################################\n",
    "# The code for changing volume is based on the code from this tutorial: https://www.youtube.com/watch?v=9iEPzbG-xLE\n",
    "def hand_change_volume(hand): \n",
    "    devices = AudioUtilities.GetSpeakers()\n",
    "    interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "    volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "    smoothing = 5\n",
    "    \n",
    "    x4, y4 = int((hand.landmark[4].x)*cw), int((hand.landmark[4].y)*ch)\n",
    "    x8, y8 = int((hand.landmark[8].x)*cw), int((hand.landmark[8].y)*ch)\n",
    "    \n",
    "    cv2.line(image, (x4, y4), (x8, y8),(255,255,255),3)\n",
    "    length = (get_distance(4, 8, hand,xyz=False) / get_distance(5, 8, hand, xyz=False))*100                     \n",
    "    vol_bar = np.interp(length, [17,175],[400, 150])\n",
    "    vol_per = np.interp(length,[17,175],[0,100])\n",
    "    vol_per = smoothing * round(vol_per/smoothing)\n",
    "    \n",
    "    volume.SetMasterVolumeLevelScalar(vol_per/100, None)\n",
    "\n",
    "    cv2.rectangle(image, (40, 150), (80, 425),(255,255,255), 3)\n",
    "    cv2.rectangle(image, (40, int(vol_bar)), (80, 425),(255,255,255), cv2.FILLED)\n",
    "    curr_vol = int(volume.GetMasterVolumeLevelScalar()*100)\n",
    "    cv2.putText(image,f'{int(curr_vol)}%',(40,140), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255),3)\n",
    "####################################################################################################    \n",
    "    \n",
    "def hand_scroll(hand):\n",
    "    smoothing = 700\n",
    "    bounds_x = cw/3\n",
    "    bounds_y = ch/3\n",
    "    \n",
    "    x, y = hand.landmark[9].x, hand.landmark[9].y\n",
    "    \n",
    "    x = np.interp(x*cw, (bounds_x,cw-bounds_x),(0,sw))\n",
    "    y = np.interp(y*ch, (bounds_y,ch-bounds_y),(0,sh))\n",
    "\n",
    "    x_rel_center = (sw/2 - x)/smoothing\n",
    "    y_rel_center = (sh/2 - y)/smoothing\n",
    "    \n",
    "    mouse.scroll(x_rel_center, y_rel_center)\n",
    "####################################################################################################    \n",
    "    \n",
    "def hand_zoom(hand, prev_y):\n",
    "    y = hand.landmark[9].y\n",
    "    \n",
    "    if y < prev_y:\n",
    "        with keyboard.pressed(Key.ctrl):\n",
    "            keyboard.press('=')\n",
    "            keyboard.release('=')  \n",
    "\n",
    "    if y > prev_y:\n",
    "        with keyboard.pressed(Key.ctrl):\n",
    "            keyboard.press('-')\n",
    "            keyboard.release('-')  \n",
    "            \n",
    "    next_zoom = time.time() + 1\n",
    "\n",
    "    return y, next_zoom\n",
    "\n",
    "####################################################################################################\n",
    "def hand_nonmode_event(lgesture, rgesture):\n",
    "    event_text = ''\n",
    "    gesture = None\n",
    "    \n",
    "    # Can specifiy if the gesture should only be read from the left or right hand, or either hand\n",
    "    if rgesture:\n",
    "        gesture = rgesture    \n",
    "    if lgesture:\n",
    "        gesture = rgesture # If both hands are up, this prioritizes the left hand for either-hand gestures\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Pause (spacebar):\n",
    "    if gesture in alt_tab_gesture:\n",
    "        with keyboard.pressed(Key.alt):\n",
    "            keyboard.press(Key.tab)\n",
    "            keyboard.release(Key.tab) \n",
    "        event_text = 'alt tab'\n",
    "        \n",
    "        \n",
    "    # Alt-tab        \n",
    "    if gesture in pause_gesture:\n",
    "        keyboard.press(Key.space)\n",
    "        keyboard.release(Key.space)   \n",
    "        pause_mode = False\n",
    "        pause_delay = time.time()  \n",
    "        event_text = 'pause'\n",
    "        \n",
    "        \n",
    "    # Cycling through tabs, opening or closing tabs:\n",
    "    if rgesture in tab1_gesture:\n",
    "        with keyboard.pressed(Key.ctrl):\n",
    "            keyboard.press('1')\n",
    "            keyboard.release('1')  \n",
    "        event_text = 'go to tab 1'\n",
    "    if rgesture in tab2_gesture:\n",
    "        with keyboard.pressed(Key.ctrl):\n",
    "            keyboard.press('2')\n",
    "            keyboard.release('2') \n",
    "        event_text = 'go to tab 2'\n",
    "    if rgesture in tab3_gesture:\n",
    "        with keyboard.pressed(Key.ctrl):\n",
    "            keyboard.press('3')\n",
    "            keyboard.release('3') \n",
    "        event_text = 'go to tab 3'\n",
    "    if rgesture in tablast_gesture:\n",
    "        with keyboard.pressed(Key.ctrl):\n",
    "            keyboard.press('9')\n",
    "            keyboard.release('9')  \n",
    "        event_text = 'go to the last tab'\n",
    "        \n",
    "    if rgesture in backward_tab_gesture:\n",
    "        with keyboard.pressed(Key.ctrl):\n",
    "            with keyboard.pressed(Key.shift):\n",
    "                keyboard.press(Key.tab)\n",
    "        keyboard.release(Key.tab)      \n",
    "        event_text = 'cycle backward through tabs'    \n",
    "    if rgesture in forward_tab_gesture:\n",
    "        with keyboard.pressed(Key.ctrl):\n",
    "            keyboard.press(Key.tab)\n",
    "        keyboard.release(Key.tab)  \n",
    "        event_text = 'cycle forward through tabs'        \n",
    "        \n",
    "    if rgesture in open_tab_gesture:  # I've found that this one doesn't get captured too efficiently\n",
    "        with keyboard.pressed(Key.ctrl):\n",
    "            keyboard.press('t')\n",
    "            keyboard.release('t') \n",
    "        event_text = 'open new tab'\n",
    "            \n",
    "    if rgesture in close_tab_gesture: # found this one to be finicky, too \n",
    "        with keyboard.pressed(Key.ctrl):\n",
    "            keyboard.press('w')\n",
    "            keyboard.release('w') \n",
    "        event_text = 'close tab or window'\n",
    "                \n",
    "            \n",
    "    # Going forward or back one page in a browser:\n",
    "    if lgesture in browser_forward_gesture:\n",
    "        with keyboard.pressed(Key.alt):\n",
    "            keyboard.press(Key.right)\n",
    "            keyboard.release(Key.right) \n",
    "        event_text = 'go forward'\n",
    "            \n",
    "    if lgesture in browser_back_gesture:\n",
    "        with keyboard.pressed(Key.alt):\n",
    "            keyboard.press(Key.left)\n",
    "            keyboard.release(Key.left) \n",
    "        event_text = 'go back'\n",
    "        \n",
    "            \n",
    "    # Links:\n",
    "    if rgesture in link1_gesture:\n",
    "        webbrowser.open('https://google.github.io/mediapipe/solutions/hands.html')\n",
    "        event_text = 'open link'\n",
    "    if rgesture in link2_gesture:\n",
    "        webbrowser.open('https://opencv.org/')  \n",
    "        event_text = 'open link'\n",
    "    if rgesture in link3_gesture:\n",
    "        os.startfile('example file.txt') \n",
    "        event_text = 'open file'\n",
    "    if rgesture in link4_gesture:\n",
    "        webbrowser.open('https://www.youtube.com/watch?v=dQw4w9WgXcQ')\n",
    "        event_text = 'open link'\n",
    "            \n",
    "    return event_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will turn on your webcam and will run the gesture recognition events. If you would like to close the OpenCV window that opens up, press \"q\" or do the end gesture in front of your webcam (currently set as a Vulcan salute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cw, ch = 640, 480 #camera resolution\n",
    "sw, sh = autopy.screen.size() #screen resolution\n",
    "cap.set(3,cw)\n",
    "cap.set(4,ch)\n",
    "keyboard = Controller()\n",
    "mouse = pynput.mouse.Controller()\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "mp_hands = mp.solutions.hands \n",
    "\n",
    "#############################################################################################\n",
    "mouse_mode, volume_mode, end_mode, scroll_mode, zoom_mode = False, False, False, False, False \n",
    "\n",
    "recorded_time, event_time, zoom_time = 0, 0, 0\n",
    "left_gestures, right_gestures = [], []\n",
    "event_text = ''\n",
    "get_time = True\n",
    "skipped = False\n",
    "time_delay = 0.25\n",
    "prev_x, prev_y = sw/2, sh/2\n",
    "\n",
    "\n",
    "nones = lambda n: [None for _ in range(n)]\n",
    "[cl_s, \n",
    " cr_s, \n",
    " cl_m, \n",
    " cr_m, \n",
    " cl_l, \n",
    " cr_l,\n",
    " lgesture,\n",
    " rgesture] = nones(8)\n",
    "#############################################################################################\n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5, max_num_hands=1) as hands: #detection: threshold for initial detection, tracking: threshold for tracking after detection, default max num of hands = 2\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "    \n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #feed needs to be RGB to work with mediapipe     \n",
    "        image = cv2.flip(image, 1) #flips the image on the horizontal\n",
    "        image.flags.writeable = False \n",
    "        results = hands.process(image) \n",
    "        image.flags.writeable = True \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "\n",
    "        \n",
    "        if results.multi_hand_landmarks: \n",
    "            for hand, side in zip(results.multi_hand_landmarks, results.multi_handedness): \n",
    "                # Draw all hand landmarks:\n",
    "                mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS,\n",
    "                                          mp_drawing.DrawingSpec(color=(255,255,255), thickness=1, circle_radius=5), #circles\n",
    "                                          mp_drawing.DrawingSpec(color=(255,255,255), thickness=1), #lines\n",
    "                                          )\n",
    "                #####################################################################\n",
    "                handedness = side.classification[0].label                 \n",
    "                if side.classification[0].score > 0.9995: #reduces noise   \n",
    "                    skipped = False\n",
    "                     \n",
    "                    if handedness == 'Left':      \n",
    "                        coords_list = get_relative_coords(hand)\n",
    "                        lgesture = get_gesture(coords_list)\n",
    "                        \n",
    "                        if volume_mode == True:\n",
    "                            volume_mode = False\n",
    "                    if handedness == 'Right':\n",
    "                        coords_list = get_relative_coords(hand)\n",
    "                        coords_list[:20] = -coords_list[:20]\n",
    "                        rgesture = get_gesture(coords_list) \n",
    "                    \n",
    "                        if mouse_mode == True:   \n",
    "                            prev_x, prev_y = hand_mouse(hand, image, prev_x, prev_y)\n",
    "                            event_time = time.time() + 1\n",
    "                            event_text = 'mouse mode'\n",
    "                        if scroll_mode == True:\n",
    "                            hand_scroll(hand)\n",
    "                            event_time = time.time() + 1\n",
    "                        if (zoom_mode == True)& ((time.time() - zoom_time) > 0):\n",
    "                            prev_y, zoom_time = hand_zoom(hand, prev_y)\n",
    "                            event_time = time.time()+ 1\n",
    "                            event_text = 'zoom mode'\n",
    "                        if volume_mode == True: \n",
    "                            hand_change_volume(hand)\n",
    "                            event_time = time.time() + 1\n",
    "                            event_text = 'volume mode'\n",
    "                else:\n",
    "                    skipped = True\n",
    "                \n",
    "                ##################################################################### \n",
    "            \n",
    "        ###################################################################################### \n",
    "        if skipped == False:\n",
    "            if get_time == True:\n",
    "                recorded_time = time.time()\n",
    "                get_time = False\n",
    "            if (get_time == False) & ((time.time()- recorded_time) > time_delay):\n",
    "                get_time = True          \n",
    "\n",
    "                if 'Left' in str(results.multi_handedness): #if left hand is on screen\n",
    "                    left_gestures.append(lgesture)\n",
    "                else:\n",
    "                    left_gestures = []\n",
    "                    lgesture, cl_s, cl_m, cl_l = None, None, None, None\n",
    "\n",
    "                if 'Right' in str(results.multi_handedness): #if right hand is on screen\n",
    "                    right_gestures.append(rgesture)\n",
    "                else:\n",
    "                    right_gestures = []\n",
    "                    rgesture, cr_s, cr_m, cr_l = None, None, None, None\n",
    "\n",
    "\n",
    "            if len(left_gestures) > 3:\n",
    "                cl_s, cl_m, cl_l = get_gesture_confirmation(left_gestures)    \n",
    "                \n",
    "\n",
    "            if len(right_gestures) > 3:\n",
    "                cr_s, cr_m, cr_l = get_gesture_confirmation(right_gestures)\n",
    "\n",
    "            ######################################################################################## \n",
    "\n",
    "            #############################################################################################################\n",
    "     \n",
    "            if (cr_s in mouse_gesture) & (mouse_mode == False) & (cl_m == None):\n",
    "                mouse_mode = True\n",
    "                event_text = 'mouse mode'\n",
    "            if (mouse_mode == True) & ((not cr_s in mouse_gesture) | (not cl_m == None)):\n",
    "                mouse_mode = False\n",
    "                event_text = ''\n",
    "                \n",
    "            if (cr_m in volume_gesture) & (volume_mode == False) & (cl_m == None) & ((time.time() - event_time) > 1):\n",
    "                volume_mode = True\n",
    "                event_text = 'volume mode'\n",
    "            if (volume_mode == True)&((not cr_s in volume_gesture) | (not cl_m == None)):\n",
    "                volume_mode = False\n",
    "                event_text = ''\n",
    "\n",
    "            if (cr_m in zoom_gesture) & (zoom_mode == False):\n",
    "                zoom_mode = True\n",
    "                event_text = 'zoom mode'\n",
    "            if (zoom_mode == True) & (not cr_s in zoom_gesture):\n",
    "                zoom_mode = False\n",
    "                event_text = ''\n",
    " \n",
    "            if (cr_m in scroll_gesture) & (scroll_mode == False):\n",
    "                scroll_mode = True\n",
    "                event_text = 'scroll mode'\n",
    "            if (scroll_mode == True) & (not cr_s in scroll_gesture):\n",
    "                scroll_mode = False\n",
    "                event_text = ''      \n",
    "    \n",
    "    \n",
    "\n",
    "            if ((not cl_l == None) | (not cr_l == None)):\n",
    "                if (time.time() - event_time) > 1.5:\n",
    "                    event_text = hand_nonmode_event(cl_l, cr_l)\n",
    "                    event_time = time.time()\n",
    "                    \n",
    "            if (cr_l in end_gesture):\n",
    "                end_mode = True\n",
    "                event_text = 'bye'   \n",
    "\n",
    "                    \n",
    "#             ###############################################################################################################\n",
    "            if ((not lgesture == None) & (not rgesture == None)):\n",
    "                cv2.putText(image, 'Left hand: ' + labels[lgesture], [25,45], cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                cv2.putText(image, 'Right hand: ' + labels[rgesture], [25,85], cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            else:\n",
    "                if not lgesture == None:\n",
    "                    cv2.putText(image, labels[lgesture], [25,45], cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                if not rgesture == None:\n",
    "                    cv2.putText(image, labels[rgesture], [25,45], cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            if event_text:\n",
    "                cv2.putText(image, event_text, [25,ch-10], cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        if (time.time() - event_time) > 2:\n",
    "            event_text = ''\n",
    "                \n",
    "        cv2.imshow('Gestures',image)     \n",
    "        \n",
    "        if (cv2.waitKey(10) & 0xFF == ord('q')) | (end_mode == True): \n",
    "            break              \n",
    "                \n",
    "cap.release() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
