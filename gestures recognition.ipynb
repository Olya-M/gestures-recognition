{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to import the different modules used in this notebook. In order to download any that aren't already installed, please run \"pip install [module name]\" in a new cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "\n",
    "import autopy\n",
    "import pynput\n",
    "from pynput.keyboard import Key, Controller\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, OpenCV will detect video from your webcam, which MediaPipe will use to detect the 21 hand landmarks. Coordinates from these landmarks will be fed into a neural network that will output its predicted hand gesture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://google.github.io/mediapipe/images/mobile/hand_landmarks.png' width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"2\">Image from https://google.github.io/mediapipe/solutions/hands.html</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following two cells before moving on to part 2 or 3 of the notebook. \n",
    "\n",
    "The cell below has all the gestures the gestures recognition model has been trained on, as well as all the gesture recognition events in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of all the gestures the gesture recognition model is currently trained on:\n",
    "labels = {\n",
    "    0: 'open palm',\n",
    "    1: 'soft palm',\n",
    "    2: 'palm rotated inward',\n",
    "    3: 'fist',\n",
    "    4: 'fist inward',\n",
    "    5: 'pinch',\n",
    "    6: 'open pinch',\n",
    "    7: 'thumbs up',\n",
    "    8: 'thumbs down',\n",
    "    9: 'spock',\n",
    "    10: 'heart',\n",
    "    11: 'ily',\n",
    "    12: 'shaka',\n",
    "    13: 'metal',\n",
    "    14: 'one finger up',\n",
    "    15: 'two fingers up',\n",
    "    16: 'three fingers up',\n",
    "    17: 'four fingers up',\n",
    "    18: 'up pinch one',\n",
    "    19: 'up pinch two',\n",
    "    20: 'up pinch three',\n",
    "    21: 'up pinch four',\n",
    "    22: 'mid pinch one',\n",
    "    23: 'mid pinch two',\n",
    "    24: 'mid pinch three',\n",
    "    25: 'mid pinch four'\n",
    "}\n",
    "\n",
    "# Computer events assigned to different gestures:\n",
    "palm = [0,1]\n",
    "mouse_gesture = palm\n",
    "stop_gesture = [3]\n",
    "volume_gesture = [5,6]\n",
    "end_gesture = [9]\n",
    "zoom_gesture = [11]\n",
    "scroll_gesture = [12]\n",
    "\n",
    "alt_tab_gesture = [2]\n",
    "pause_gesture = [10]\n",
    "tab1_gesture = [14]\n",
    "tab2_gesture = [15]\n",
    "tab3_gesture = [16]\n",
    "tablast_gesture = [17]\n",
    "open_tab_gesture = [20]\n",
    "close_tab_gesture = [21]\n",
    "forward_tab_gesture = [19]\n",
    "backward_tab_gesture = [18]\n",
    "browser_forward_gesture = [18]\n",
    "browser_back_gesture = [19]\n",
    "link1_gesture = [22]\n",
    "link2_gesture = [23]\n",
    "link3_gesture = [24]\n",
    "link4_gesture = [25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell's function will return flattened x, y, and z coordinates for the hand landmarks relative to the wrist's coordinates. This is how the coordinates will be fed into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_coords(hand):\n",
    "    rel_coords = []\n",
    "    \n",
    "    for lmk in hand.landmark:\n",
    "        coords = (hand.landmark[0].x- lmk.x, \n",
    "                hand.landmark[0].y - lmk.y,\n",
    "                hand.landmark[0].z - lmk.z)\n",
    "        rel_coords.append(coords)\n",
    "    \n",
    "    rel_coords = (np.array(rel_coords)).flatten('F')\n",
    "            \n",
    "    return rel_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to train the model with your own hand gestures, please move on to part 2 of the notebook. If you would like to just run the model with the pretrained weights, please skip ahead to part 3 of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i. Collecting coordinates for gestures from your webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialize an empty list for all the training data and how long you would like each gesture recording to take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "training_time = 20 # For how long the current gesture will be recorded (in seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, set the label for what gesture you would currently like to collect coordinates for. Run the following cell each time you pick a new gesture. When I trained the model, for each gesture, I recorded one set of gestures for my right hand, and then for my right hand, and then moved on to the next gestures label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_label = 26\n",
    "print('Ready to record the following gesture: ' + labels[curr_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will run your webcam and will store the collected coords in coords_list_both. OpenCV handles the video capture. The OpenCV window will close by itself once the set training_time has passed, or earlier if you press 'q'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) #might have to change this if you have more than one webcam\n",
    "cw, ch = 640, 480 #camera resolution\n",
    "sw, sh = autopy.screen.size() #screen resolution\n",
    "cap.set(3,cw)\n",
    "cap.set(4,ch)\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "mp_hands = mp.solutions.hands \n",
    "coords_list_both = []\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5, max_num_hands=1) as hands: \n",
    "    # May need to reduce min_detection_confidence when recording certain gestures\n",
    "    # Only one hand is intended to be noted down per gesture recording for training, so the maximum number of hands here is 1\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "    \n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  \n",
    "        image = cv2.flip(image, 1) \n",
    "        image.flags.writeable = False \n",
    "        results = hands.process(image) \n",
    "        image.flags.writeable = True \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "        \n",
    "        if results.multi_hand_landmarks: \n",
    "            for hand, side in zip(results.multi_hand_landmarks, results.multi_handedness): \n",
    "                # Draw all hand landmarks:\n",
    "                mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS,\n",
    "                                          mp_drawing.DrawingSpec(color=(255,255,255), thickness=1, circle_radius=5), \n",
    "                                          mp_drawing.DrawingSpec(color=(255,255,255), thickness=1), \n",
    "                                          )\n",
    "                #####################################################################\n",
    "                handedness = side.classification[0].label                 \n",
    "                if side.classification[0].score > 0.9995: #reduces noise   \n",
    "                     \n",
    "                    if handedness == 'Left':      \n",
    "                        coords_list = get_relative_coords(hand)\n",
    "                        coords_list_both.append(coords_list) #could split up right and left hand for training\n",
    "\n",
    "                    if handedness == 'Right':\n",
    "                        coords_list = get_relative_coords(hand)\n",
    "                        coords_list[:20] = -coords_list[:20] #the right hand is treated as a flipped left hand for the sake of the model    \n",
    "                        coords_list_both.append(coords_list)\n",
    "                \n",
    "                ##################################################################### \n",
    "                          \n",
    "        cv2.imshow('Gestures',image)           \n",
    "        if (cv2.waitKey(10) & 0xFF == ord('q'))|((time.time() - start_time) > training_time): \n",
    "            break\n",
    "                \n",
    "cap.release() \n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "coords_list_both = np.insert(coords_list_both, 0, curr_label, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell for each gesture recording or add the code to the previous cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.append(coords_list_both) \n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After collecting the coords from all of your gestures, run the following cell to put your data in a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_training_data =[x for sublist in training_data for x in sublist]\n",
    "df = pd.DataFrame(flattened_training_data, columns = ['Label', *list(range(0, 63))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following cell if you would like to append your collected coords to mine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = df\n",
    "# df = (pd.read_csv('training_data.csv')).append(new_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to see what your data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.041926</td>\n",
       "      <td>-0.064393</td>\n",
       "      <td>-0.077695</td>\n",
       "      <td>-0.100478</td>\n",
       "      <td>-0.020028</td>\n",
       "      <td>-0.026275</td>\n",
       "      <td>-0.029684</td>\n",
       "      <td>-0.033808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094016</td>\n",
       "      <td>0.112747</td>\n",
       "      <td>0.049911</td>\n",
       "      <td>0.072864</td>\n",
       "      <td>0.094560</td>\n",
       "      <td>0.112693</td>\n",
       "      <td>0.055764</td>\n",
       "      <td>0.081397</td>\n",
       "      <td>0.099972</td>\n",
       "      <td>0.112621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.055530</td>\n",
       "      <td>-0.092898</td>\n",
       "      <td>-0.115531</td>\n",
       "      <td>-0.141731</td>\n",
       "      <td>-0.040270</td>\n",
       "      <td>-0.063163</td>\n",
       "      <td>-0.075475</td>\n",
       "      <td>-0.085149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057199</td>\n",
       "      <td>0.078605</td>\n",
       "      <td>0.036256</td>\n",
       "      <td>0.050221</td>\n",
       "      <td>0.068569</td>\n",
       "      <td>0.084626</td>\n",
       "      <td>0.051791</td>\n",
       "      <td>0.071071</td>\n",
       "      <td>0.085861</td>\n",
       "      <td>0.096988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.058933</td>\n",
       "      <td>-0.097290</td>\n",
       "      <td>-0.120274</td>\n",
       "      <td>-0.148960</td>\n",
       "      <td>-0.043884</td>\n",
       "      <td>-0.067256</td>\n",
       "      <td>-0.079459</td>\n",
       "      <td>-0.088954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051335</td>\n",
       "      <td>0.071981</td>\n",
       "      <td>0.032002</td>\n",
       "      <td>0.043575</td>\n",
       "      <td>0.061155</td>\n",
       "      <td>0.075661</td>\n",
       "      <td>0.046285</td>\n",
       "      <td>0.063098</td>\n",
       "      <td>0.076718</td>\n",
       "      <td>0.086555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.062528</td>\n",
       "      <td>-0.101911</td>\n",
       "      <td>-0.125646</td>\n",
       "      <td>-0.156448</td>\n",
       "      <td>-0.044332</td>\n",
       "      <td>-0.069646</td>\n",
       "      <td>-0.083451</td>\n",
       "      <td>-0.093932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046991</td>\n",
       "      <td>0.067786</td>\n",
       "      <td>0.029085</td>\n",
       "      <td>0.039506</td>\n",
       "      <td>0.056380</td>\n",
       "      <td>0.070195</td>\n",
       "      <td>0.042327</td>\n",
       "      <td>0.058426</td>\n",
       "      <td>0.071323</td>\n",
       "      <td>0.080170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.060706</td>\n",
       "      <td>-0.100116</td>\n",
       "      <td>-0.123723</td>\n",
       "      <td>-0.152498</td>\n",
       "      <td>-0.041669</td>\n",
       "      <td>-0.067570</td>\n",
       "      <td>-0.082614</td>\n",
       "      <td>-0.093875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064941</td>\n",
       "      <td>0.088373</td>\n",
       "      <td>0.040920</td>\n",
       "      <td>0.057121</td>\n",
       "      <td>0.078812</td>\n",
       "      <td>0.096684</td>\n",
       "      <td>0.058394</td>\n",
       "      <td>0.079690</td>\n",
       "      <td>0.095399</td>\n",
       "      <td>0.106746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25238</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.029870</td>\n",
       "      <td>-0.044996</td>\n",
       "      <td>-0.018196</td>\n",
       "      <td>0.009353</td>\n",
       "      <td>-0.074637</td>\n",
       "      <td>-0.098222</td>\n",
       "      <td>-0.110564</td>\n",
       "      <td>-0.118064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052539</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.015566</td>\n",
       "      <td>0.031339</td>\n",
       "      <td>0.057968</td>\n",
       "      <td>0.079122</td>\n",
       "      <td>0.013185</td>\n",
       "      <td>0.032928</td>\n",
       "      <td>0.056452</td>\n",
       "      <td>0.072856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25239</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.029561</td>\n",
       "      <td>-0.044932</td>\n",
       "      <td>-0.020767</td>\n",
       "      <td>0.004882</td>\n",
       "      <td>-0.075192</td>\n",
       "      <td>-0.097627</td>\n",
       "      <td>-0.109392</td>\n",
       "      <td>-0.116656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060101</td>\n",
       "      <td>0.076826</td>\n",
       "      <td>0.019354</td>\n",
       "      <td>0.034823</td>\n",
       "      <td>0.062218</td>\n",
       "      <td>0.084348</td>\n",
       "      <td>0.014773</td>\n",
       "      <td>0.033055</td>\n",
       "      <td>0.055936</td>\n",
       "      <td>0.072838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25240</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.029826</td>\n",
       "      <td>-0.044019</td>\n",
       "      <td>-0.017242</td>\n",
       "      <td>0.009939</td>\n",
       "      <td>-0.073680</td>\n",
       "      <td>-0.096601</td>\n",
       "      <td>-0.108968</td>\n",
       "      <td>-0.116440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052541</td>\n",
       "      <td>0.067963</td>\n",
       "      <td>0.014630</td>\n",
       "      <td>0.028576</td>\n",
       "      <td>0.053562</td>\n",
       "      <td>0.073606</td>\n",
       "      <td>0.010056</td>\n",
       "      <td>0.026351</td>\n",
       "      <td>0.047288</td>\n",
       "      <td>0.062207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25241</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.030386</td>\n",
       "      <td>-0.044888</td>\n",
       "      <td>-0.017833</td>\n",
       "      <td>0.010559</td>\n",
       "      <td>-0.072931</td>\n",
       "      <td>-0.094612</td>\n",
       "      <td>-0.106458</td>\n",
       "      <td>-0.114058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046301</td>\n",
       "      <td>0.060641</td>\n",
       "      <td>0.013730</td>\n",
       "      <td>0.026451</td>\n",
       "      <td>0.051075</td>\n",
       "      <td>0.070704</td>\n",
       "      <td>0.011141</td>\n",
       "      <td>0.026840</td>\n",
       "      <td>0.047899</td>\n",
       "      <td>0.063093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25242</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>-0.043577</td>\n",
       "      <td>-0.017854</td>\n",
       "      <td>0.010182</td>\n",
       "      <td>-0.070311</td>\n",
       "      <td>-0.090207</td>\n",
       "      <td>-0.101835</td>\n",
       "      <td>-0.111116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080041</td>\n",
       "      <td>0.098675</td>\n",
       "      <td>0.033308</td>\n",
       "      <td>0.054387</td>\n",
       "      <td>0.079949</td>\n",
       "      <td>0.100864</td>\n",
       "      <td>0.030802</td>\n",
       "      <td>0.052233</td>\n",
       "      <td>0.073307</td>\n",
       "      <td>0.089665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25243 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label    0         1         2         3         4         5         6  \\\n",
       "0        0.0 -0.0 -0.041926 -0.064393 -0.077695 -0.100478 -0.020028 -0.026275   \n",
       "1        0.0 -0.0 -0.055530 -0.092898 -0.115531 -0.141731 -0.040270 -0.063163   \n",
       "2        0.0 -0.0 -0.058933 -0.097290 -0.120274 -0.148960 -0.043884 -0.067256   \n",
       "3        0.0 -0.0 -0.062528 -0.101911 -0.125646 -0.156448 -0.044332 -0.069646   \n",
       "4        0.0 -0.0 -0.060706 -0.100116 -0.123723 -0.152498 -0.041669 -0.067570   \n",
       "...      ...  ...       ...       ...       ...       ...       ...       ...   \n",
       "25238   25.0  0.0 -0.029870 -0.044996 -0.018196  0.009353 -0.074637 -0.098222   \n",
       "25239   25.0  0.0 -0.029561 -0.044932 -0.020767  0.004882 -0.075192 -0.097627   \n",
       "25240   25.0  0.0 -0.029826 -0.044019 -0.017242  0.009939 -0.073680 -0.096601   \n",
       "25241   25.0  0.0 -0.030386 -0.044888 -0.017833  0.010559 -0.072931 -0.094612   \n",
       "25242   25.0  0.0 -0.027805 -0.043577 -0.017854  0.010182 -0.070311 -0.090207   \n",
       "\n",
       "              7         8  ...        53        54        55        56  \\\n",
       "0     -0.029684 -0.033808  ...  0.094016  0.112747  0.049911  0.072864   \n",
       "1     -0.075475 -0.085149  ...  0.057199  0.078605  0.036256  0.050221   \n",
       "2     -0.079459 -0.088954  ...  0.051335  0.071981  0.032002  0.043575   \n",
       "3     -0.083451 -0.093932  ...  0.046991  0.067786  0.029085  0.039506   \n",
       "4     -0.082614 -0.093875  ...  0.064941  0.088373  0.040920  0.057121   \n",
       "...         ...       ...  ...       ...       ...       ...       ...   \n",
       "25238 -0.110564 -0.118064  ...  0.052539  0.068158  0.015566  0.031339   \n",
       "25239 -0.109392 -0.116656  ...  0.060101  0.076826  0.019354  0.034823   \n",
       "25240 -0.108968 -0.116440  ...  0.052541  0.067963  0.014630  0.028576   \n",
       "25241 -0.106458 -0.114058  ...  0.046301  0.060641  0.013730  0.026451   \n",
       "25242 -0.101835 -0.111116  ...  0.080041  0.098675  0.033308  0.054387   \n",
       "\n",
       "             57        58        59        60        61        62  \n",
       "0      0.094560  0.112693  0.055764  0.081397  0.099972  0.112621  \n",
       "1      0.068569  0.084626  0.051791  0.071071  0.085861  0.096988  \n",
       "2      0.061155  0.075661  0.046285  0.063098  0.076718  0.086555  \n",
       "3      0.056380  0.070195  0.042327  0.058426  0.071323  0.080170  \n",
       "4      0.078812  0.096684  0.058394  0.079690  0.095399  0.106746  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "25238  0.057968  0.079122  0.013185  0.032928  0.056452  0.072856  \n",
       "25239  0.062218  0.084348  0.014773  0.033055  0.055936  0.072838  \n",
       "25240  0.053562  0.073606  0.010056  0.026351  0.047288  0.062207  \n",
       "25241  0.051075  0.070704  0.011141  0.026840  0.047899  0.063093  \n",
       "25242  0.079949  0.100864  0.030802  0.052233  0.073307  0.089665  \n",
       "\n",
       "[25243 rows x 64 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to save your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'training_data_new'\n",
    "df.to_csv(filename+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ii. Training a neural network for gesture recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to prepare a custom dataset from the training data that was just collected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "class HandGesturesDataset(Dataset):\n",
    "    def __init__(self, ):\n",
    "        xy = np.loadtxt(open(filename+'.csv','rb'), delimiter=',', dtype=np.float32, skiprows=1)\n",
    "        self.x = torch.from_numpy(xy[:,1:])\n",
    "        self.y = (torch.from_numpy(xy[:,0])).type(torch.LongTensor)\n",
    "        self.n_samples = xy.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "dataset = HandGesturesDataset()\n",
    "indices = list(range(len(dataset)))\n",
    "split = int(np.floor(0.2 * len(dataset)))\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to initialize the neural network used in this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_size = 63\n",
    "num_classes = len(labels)\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "class NeuralNet(pl.LightningModule):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(256, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        return out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"loss\", loss)        \n",
    "        return loss\n",
    "  \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log('val_loss', loss) \n",
    "           \n",
    "    def train_dataloader(self):\n",
    "        train_dataloader = DataLoader(dataset=dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataloader = DataLoader(dataset=dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "        return val_dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to train the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "trainer = Trainer(auto_lr_find=True, max_epochs=num_epochs, gpus=1, fast_dev_run=False)\n",
    "model = NeuralNet(input_size, num_classes)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment this if you would like to run TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %load_ext tensorboard\n",
    "# %tensorboard --logdir ./lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment this if you would like to save your model's weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'gestures_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Running the model for gesture recognition and gesture recognition events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the neural network if you haven't done so already in part 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_size = 63\n",
    "num_classes = len(labels)\n",
    "\n",
    "class NeuralNet(pl.LightningModule):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(256, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell if you would like to use the pretrained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNet(input_size, num_classes)\n",
    "model.load_state_dict(torch.load('100epochs.pth')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to define some general helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feeds hand coordinates through the trained model to get its predicted hand gesture\n",
    "def get_gesture(coords_list):\n",
    "    model_out = model(torch.tensor(coords_list.astype(np.float32)))\n",
    "    gesture = torch.argmax(model_out).item()\n",
    "    return gesture\n",
    "\n",
    "\n",
    "# Returns a distance in 2D or in 3D between two points\n",
    "def get_distance(a, b, hand, xyz=True):\n",
    "    distance = 0\n",
    "    if xyz == True:\n",
    "        distance = np.cbrt((hand.landmark[a].x - hand.landmark[b].x)**2 \n",
    "                           + (hand.landmark[a].y - hand.landmark[b].y)**2\n",
    "                           + (hand.landmark[a].z - hand.landmark[b].z)**2)\n",
    "    else:\n",
    "        distance = np.sqrt((hand.landmark[a].x - hand.landmark[b].x)**2 \n",
    "                           + (hand.landmark[a].y - hand.landmark[b].y)**2)\n",
    "    return distance\n",
    "\n",
    "# Returns gestures if they match ones recorded in previous time steps (to to be able to modulate how responsive the different gesture events should be)\n",
    "def get_gesture_confirmation(gestures):\n",
    "    short, med, long = None, None, None\n",
    "    if gestures[-1] == gestures[-2]:\n",
    "        short = gestures[-1]\n",
    "        if gestures[-1] == gestures[-2] == gestures[-3]:\n",
    "            med = gestures[-1]\n",
    "            if gestures[-1] == gestures[-2] == gestures[-3] == gestures[-4]:\n",
    "                long = gestures[-1]\n",
    "    return short, med, long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to define functions for running different hand gesture events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code for the hand mouse is in part based on the code from this tutorial: https://www.youtube.com/watch?v=8gPONnGIPgw\n",
    "def hand_mouse(hand, image, prev_x, prev_y):\n",
    "    smoothing = 3\n",
    "    bounds_x = cw/3\n",
    "    bounds_y = ch/3\n",
    "    \n",
    "    x, y = hand.landmark[9].x, hand.landmark[9].y #the mouse coords here are obtained from the middle finger MCP to keep the pointer stable when clicking\n",
    "    \n",
    "    x = np.interp(x*cw, (bounds_x,cw-bounds_x),(0,sw))\n",
    "    y = np.interp(y*ch, (bounds_y,ch-bounds_y),(0,sh))\n",
    "\n",
    "    curr_x = prev_x + (x - prev_x)/smoothing\n",
    "    curr_y = prev_y + (y - prev_y)/smoothing\n",
    "    \n",
    "    autopy.mouse.move(curr_x, curr_y)\n",
    "    \n",
    "    # Distance between the tips of the index and middle fingers divided by index finger length to control for distance away from the camera\n",
    "    click_distance = (get_distance(8, 12, hand,xyz=False) / get_distance(5, 8, hand, xyz=False)) \n",
    "    if click_distance < 0.2: \n",
    "        autopy.mouse.click()\n",
    "\n",
    "    return curr_x, curr_y\n",
    "####################################################################################################\n",
    "# The code for changing volume is based on the code from this tutorial: https://www.youtube.com/watch?v=9iEPzbG-xLE\n",
    "def hand_change_volume(hand): \n",
    "    devices = AudioUtilities.GetSpeakers()\n",
    "    interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "    volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "    smoothing = 5\n",
    "    \n",
    "    x4, y4 = int((hand.landmark[4].x)*cw), int((hand.landmark[4].y)*ch)\n",
    "    x8, y8 = int((hand.landmark[8].x)*cw), int((hand.landmark[8].y)*ch)\n",
    "    \n",
    "    cv2.line(image, (x4, y4), (x8, y8),(255,255,255),3)\n",
    "    length = (get_distance(4, 8, hand,xyz=False) / get_distance(5, 8, hand, xyz=False))*100                     \n",
    "    vol_bar = np.interp(length, [17,175],[400, 150])\n",
    "    vol_per = np.interp(length,[17,175],[0,100])\n",
    "    vol_per = smoothing * round(vol_per/smoothing)\n",
    "    \n",
    "    volume.SetMasterVolumeLevelScalar(vol_per/100, None)\n",
    "\n",
    "    cv2.rectangle(image, (40, 150), (80, 425),(255,255,255), 3)\n",
    "    cv2.rectangle(image, (40, int(vol_bar)), (80, 425),(255,255,255), cv2.FILLED)\n",
    "    curr_vol = int(volume.GetMasterVolumeLevelScalar()*100)\n",
    "    cv2.putText(image,f'{int(curr_vol)}%',(40,140), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255),3)\n",
    "####################################################################################################    \n",
    "    \n",
    "def hand_scroll(hand):\n",
    "    smoothing = 700\n",
    "    bounds_x = cw/3\n",
    "    bounds_y = ch/3\n",
    "    \n",
    "    x, y = hand.landmark[9].x, hand.landmark[9].y\n",
    "    \n",
    "    x = np.interp(x*cw, (bounds_x,cw-bounds_x),(0,sw))\n",
    "    y = np.interp(y*ch, (bounds_y,ch-bounds_y),(0,sh))\n",
    "\n",
    "    x_rel_center = (sw/2 - x)/smoothing\n",
    "    y_rel_center = (sh/2 - y)/smoothing\n",
    "    \n",
    "    mouse.scroll(x_rel_center, y_rel_center)\n",
    "####################################################################################################    \n",
    "    \n",
    "def hand_zoom(hand, prev_y):\n",
    "    y = hand.landmark[9].y\n",
    "    \n",
    "    if y < prev_y:\n",
    "        with keyboard.pressed(Key.ctrl):\n",
    "            keyboard.press('=')\n",
    "            keyboard.release('=')  \n",
    "\n",
    "    if y > prev_y:\n",
    "        with keyboard.pressed(Key.ctrl):\n",
    "            keyboard.press('-')\n",
    "            keyboard.release('-')  \n",
    "            \n",
    "    next_zoom = time.time() + 1\n",
    "\n",
    "    return y, next_zoom\n",
    "\n",
    "####################################################################################################\n",
    "def hand_nonmode_event(lgesture, rgesture):\n",
    "    event_text = ''\n",
    "    gesture = None\n",
    "    \n",
    "    # Can specifiy if the gesture should only be read from the left or right hand, or either hand\n",
    "    if rgesture:\n",
    "        gesture = rgesture    \n",
    "    if lgesture:\n",
    "        gesture = rgesture # If both hands are up, this prioritizes the left hand for either-hand gestures\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Pause (spacebar):\n",
    "    if gesture in alt_tab_gesture:\n",
    "        with keyboard.pressed(Key.alt):\n",
    "            keyboard.press(Key.tab)\n",
    "            keyboard.release(Key.tab) \n",
    "        event_text = 'alt tab'\n",
    "        \n",
    "        \n",
    "    # Alt-tab        \n",
    "    if gesture in pause_gesture:\n",
    "        keyboard.press(Key.space)\n",
    "        keyboard.release(Key.space)   \n",
    "        pause_mode = False\n",
    "        pause_delay = time.time()  \n",
    "        event_text = 'pause'\n",
    "        \n",
    "        \n",
    "    # Cycling through tabs, opening or closing tabs:\n",
    "    if rgesture in tab1_gesture:\n",
    "        with keyboard.pressed(Key.ctrl):\n",
    "            keyboard.press('1')\n",
    "            keyboard.release('1')  \n",
    "        event_text = 'go to tab 1'\n",
    "    if rgesture in tab2_gesture:\n",
    "        with keyboard.pressed(Key.ctrl):\n",
    "            keyboard.press('2')\n",
    "            keyboard.release('2') \n",
    "        event_text = 'go to tab 2'\n",
    "    if rgesture in tab3_gesture:\n",
    "        with keyboard.pressed(Key.ctrl):\n",
    "            keyboard.press('3')\n",
    "            keyboard.release('3') \n",
    "        event_text = 'go to tab 3'\n",
    "    if rgesture in tablast_gesture:\n",
    "        with keyboard.pressed(Key.ctrl):\n",
    "            keyboard.press('9')\n",
    "            keyboard.release('9')  \n",
    "        event_text = 'go to the last tab'\n",
    "        \n",
    "    if rgesture in backward_tab_gesture:\n",
    "        with keyboard.pressed(Key.ctrl):\n",
    "            with keyboard.pressed(Key.shift):\n",
    "                keyboard.press(Key.tab)\n",
    "        keyboard.release(Key.tab)      \n",
    "        event_text = 'cycle backward through tabs'    \n",
    "    if rgesture in forward_tab_gesture:\n",
    "        with keyboard.pressed(Key.ctrl):\n",
    "            keyboard.press(Key.tab)\n",
    "        keyboard.release(Key.tab)  \n",
    "        event_text = 'cycle forward through tabs'        \n",
    "        \n",
    "    if rgesture in open_tab_gesture:  # I've found that this one doesn't get captured too efficiently\n",
    "        with keyboard.pressed(Key.ctrl):\n",
    "            keyboard.press('t')\n",
    "            keyboard.release('t') \n",
    "        event_text = 'open new tab'\n",
    "            \n",
    "    if rgesture in close_tab_gesture: # found this one to be finicky, too \n",
    "        with keyboard.pressed(Key.ctrl):\n",
    "            keyboard.press('w')\n",
    "            keyboard.release('w') \n",
    "        event_text = 'close tab or window'\n",
    "                \n",
    "            \n",
    "    # Going forward or back one page in a browser:\n",
    "    if lgesture in browser_forward_gesture:\n",
    "        with keyboard.pressed(Key.alt):\n",
    "            keyboard.press(Key.right)\n",
    "            keyboard.release(Key.right) \n",
    "        event_text = 'go forward'\n",
    "            \n",
    "    if lgesture in browser_back_gesture:\n",
    "        with keyboard.pressed(Key.alt):\n",
    "            keyboard.press(Key.left)\n",
    "            keyboard.release(Key.left) \n",
    "        event_text = 'go back'\n",
    "        \n",
    "            \n",
    "    # Links:\n",
    "    if rgesture in link1_gesture:\n",
    "        webbrowser.open('https://google.github.io/mediapipe/solutions/hands.html')\n",
    "        event_text = 'open link'\n",
    "    if rgesture in link2_gesture:\n",
    "        webbrowser.open('https://opencv.org/')  \n",
    "        event_text = 'open link'\n",
    "    if rgesture in link3_gesture:\n",
    "        os.startfile('example file.txt') \n",
    "        event_text = 'open file'\n",
    "    if rgesture in link4_gesture:\n",
    "        webbrowser.open('https://www.youtube.com/watch?v=dQw4w9WgXcQ')\n",
    "        event_text = 'open link'\n",
    "            \n",
    "    return event_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python cell will turn on your webcam and will run the gesture recognition events. If you would like to close the OpenCV window that opens up, press \"q\" or do the end gesture in front of your webcam, which is currently set as a Vulcan salute (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://api.time.com/wp-content/uploads/2015/04/464967684.jpg?w=800&quality=85' width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cw, ch = 640, 480 #camera resolution\n",
    "sw, sh = autopy.screen.size() #screen resolution\n",
    "cap.set(3,cw)\n",
    "cap.set(4,ch)\n",
    "keyboard = Controller()\n",
    "mouse = pynput.mouse.Controller()\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "mp_hands = mp.solutions.hands \n",
    "\n",
    "#############################################################################################\n",
    "mouse_mode, volume_mode, end_mode, scroll_mode, zoom_mode = False, False, False, False, False \n",
    "\n",
    "recorded_time, event_time, zoom_time = 0, 0, 0\n",
    "left_gestures, right_gestures = [], []\n",
    "event_text = ''\n",
    "get_time = True\n",
    "skipped = False\n",
    "time_delay = 0.25  #this delay can be increased if you would like gesture sensivity to increase, and vice versa\n",
    "prev_x, prev_y = sw/2, sh/2\n",
    "\n",
    "\n",
    "nones = lambda n: [None for _ in range(n)]\n",
    "[cl_s, \n",
    " cr_s, \n",
    " cl_m, \n",
    " cr_m, \n",
    " cl_l, \n",
    " cr_l,\n",
    " lgesture,\n",
    " rgesture] = nones(8)\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5, max_num_hands=1) as hands: #detection: threshold for initial detection, tracking: threshold for tracking after detection, default max num of hands = 2\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "    \n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)   \n",
    "        image = cv2.flip(image, 1) \n",
    "        image.flags.writeable = False \n",
    "        results = hands.process(image) \n",
    "        image.flags.writeable = True \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  \n",
    "\n",
    "        \n",
    "        if results.multi_hand_landmarks: \n",
    "            for hand, side in zip(results.multi_hand_landmarks, results.multi_handedness): \n",
    "                # Draws the landmarks:\n",
    "                mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS,\n",
    "                                          mp_drawing.DrawingSpec(color=(255,255,255), thickness=1, circle_radius=5), \n",
    "                                          mp_drawing.DrawingSpec(color=(255,255,255), thickness=1),\n",
    "                                          )\n",
    "                #####################################################################\n",
    "                # Figures out which hand's up and sets off some gesture events\n",
    "                handedness = side.classification[0].label                 \n",
    "                if side.classification[0].score > 0.9995: #reduces noise   \n",
    "                    skipped = False\n",
    "                     \n",
    "                    if handedness == 'Left':      \n",
    "                        coords_list = get_relative_coords(hand)\n",
    "                        lgesture = get_gesture(coords_list)\n",
    "                        \n",
    "                        if volume_mode == True:\n",
    "                            volume_mode = False\n",
    "                    if handedness == 'Right':\n",
    "                        coords_list = get_relative_coords(hand)\n",
    "                        coords_list[:20] = -coords_list[:20]\n",
    "                        rgesture = get_gesture(coords_list) \n",
    "                    \n",
    "                        if mouse_mode == True:   \n",
    "                            prev_x, prev_y = hand_mouse(hand, image, prev_x, prev_y)\n",
    "                            event_time = time.time() + 1\n",
    "                            event_text = 'mouse mode'\n",
    "                        if scroll_mode == True:\n",
    "                            hand_scroll(hand)\n",
    "                            event_time = time.time() + 1\n",
    "                        if (zoom_mode == True)& ((time.time() - zoom_time) > 0):\n",
    "                            prev_y, zoom_time = hand_zoom(hand, prev_y)\n",
    "                            event_time = time.time()+ 1\n",
    "                            event_text = 'zoom mode'\n",
    "                        if volume_mode == True: \n",
    "                            hand_change_volume(hand)\n",
    "                            event_time = time.time() + 1\n",
    "                            event_text = 'volume mode'\n",
    "                else:\n",
    "                    skipped = True\n",
    "                \n",
    "                ##################################################################### \n",
    "            \n",
    "        ###################################################################################### \n",
    "        # This section records a hand gesture for each hand every time delay. Gestures consistent across multiple time steps are returned\n",
    "        if skipped == False:\n",
    "            if get_time == True:\n",
    "                recorded_time = time.time()\n",
    "                get_time = False\n",
    "            if (get_time == False) & ((time.time()- recorded_time) > time_delay):\n",
    "                get_time = True          \n",
    "\n",
    "                if 'Left' in str(results.multi_handedness): #if left hand is on screen\n",
    "                    left_gestures.append(lgesture)\n",
    "                else:\n",
    "                    left_gestures = []\n",
    "                    lgesture, cl_s, cl_m, cl_l = None, None, None, None\n",
    "\n",
    "                if 'Right' in str(results.multi_handedness): #if right hand is on screen\n",
    "                    right_gestures.append(rgesture)\n",
    "                else:\n",
    "                    right_gestures = []\n",
    "                    rgesture, cr_s, cr_m, cr_l = None, None, None, None\n",
    "\n",
    "\n",
    "            if len(left_gestures) > 3:\n",
    "                cl_s, cl_m, cl_l = get_gesture_confirmation(left_gestures) #use cl_l v.s. cl_s, for example, if you would like an event to be initiated after a gesture has been up (and captured) for a longer period of time\n",
    "                \n",
    "\n",
    "            if len(right_gestures) > 3:\n",
    "                cr_s, cr_m, cr_l = get_gesture_confirmation(right_gestures)\n",
    "\n",
    "            ######################################################################################## \n",
    "\n",
    "            #############################################################################################################\n",
    "            # This section handles the triggers for different events\n",
    "            if (cr_s in mouse_gesture) & (mouse_mode == False) & (cl_m == None):\n",
    "                mouse_mode = True\n",
    "                event_text = 'mouse mode'\n",
    "            if (mouse_mode == True) & ((not cr_s in mouse_gesture) | (not cl_m == None)):\n",
    "                mouse_mode = False\n",
    "                event_text = ''\n",
    "                \n",
    "            if (cr_m in volume_gesture) & (volume_mode == False) & (cl_m == None) & ((time.time() - event_time) > 1):\n",
    "                volume_mode = True\n",
    "                event_text = 'volume mode'\n",
    "            if (volume_mode == True)&((not cr_s in volume_gesture) | (not cl_m == None)):\n",
    "                volume_mode = False\n",
    "                event_text = ''\n",
    "\n",
    "            if (cr_m in zoom_gesture) & (zoom_mode == False):\n",
    "                zoom_mode = True\n",
    "                event_text = 'zoom mode'\n",
    "            if (zoom_mode == True) & (not cr_s in zoom_gesture):\n",
    "                zoom_mode = False\n",
    "                event_text = ''\n",
    " \n",
    "            if (cr_m in scroll_gesture) & (scroll_mode == False):\n",
    "                scroll_mode = True\n",
    "                event_text = 'scroll mode'\n",
    "            if (scroll_mode == True) & (not cr_s in scroll_gesture):\n",
    "                scroll_mode = False\n",
    "                event_text = ''      \n",
    "    \n",
    "    \n",
    "\n",
    "            if ((not cl_l == None) | (not cr_l == None)):\n",
    "                if (time.time() - event_time) > 1.5:\n",
    "                    event_text = hand_nonmode_event(cl_l, cr_l)\n",
    "                    event_time = time.time()\n",
    "                    \n",
    "            if (cr_l in end_gesture):\n",
    "                end_mode = True\n",
    "                event_text = 'bye'   \n",
    "\n",
    "                    \n",
    "            ###############################################################################################################\n",
    "            # This section handles the text that shows up on the screen\n",
    "            if ((not lgesture == None) & (not rgesture == None)):\n",
    "                cv2.putText(image, 'Left hand: ' + labels[lgesture], [25,45], cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                cv2.putText(image, 'Right hand: ' + labels[rgesture], [25,85], cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            else:\n",
    "                if not lgesture == None:\n",
    "                    cv2.putText(image, labels[lgesture], [25,45], cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                if not rgesture == None:\n",
    "                    cv2.putText(image, labels[rgesture], [25,45], cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            if event_text:\n",
    "                cv2.putText(image, event_text, [25,ch-10], cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        if (time.time() - event_time) > 2:\n",
    "            event_text = ''\n",
    "                \n",
    "        cv2.imshow('Gestures',image)     \n",
    "        \n",
    "        if (cv2.waitKey(10) & 0xFF == ord('q')) | (end_mode == True): \n",
    "            break              \n",
    "                \n",
    "cap.release() \n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
